import coconut.convenience

import os, time, datetime
from itertools import repeat
from typing import Callable
import torch as pt
from torch.utils.tensorboard import SummaryWriter
#import gym, gace
#import hace as ac

## Hyper Parameters
verbose: bool        = True            # Print verbose debug output
algorithm: str       = "sac"           # Name of used algorithm ∈  ./algorithm
num_episodes: int    = 666             # Number of episodes to play
num_steps: int       = 1               # How many steps to take in env
num_epochs:int       = 1               # How many gradient update steps
num_iterations: int  = 150             # Number of iterations
early_stop: float    = -500.0          # Early stop criterion
batch_size: int      = 256             # size of the batches during epoch
rng_seed: int        = 666             # Random seed for reproducability
max_time: float      = 20.0            # Maximum time to cut off

## ACE
ace_id: str          = "op2"           # ACE Identifier of the Environment
ace_backend: str     = "xh035"         # PDK/Technology backend of the ACE Environment
ace_variant: int     = 0               # ACE Environment variant

## SAC
γ: float             = 0.99            # Discount Factor
τ_soft: float        = 5e-3            # Smoothing Coefficient
α_const: float       = 0.2 # 0.036    # Temperature Parameter
σ_min: float         = -20             # Lower Variance Clipping
σ_max: float         = 2               # Upper Variance Clipping
ε_noise: float       = 1e-6            # Noisy action
reward_scale: float  = 5.0             # Reward Scaling Factor

## PER
buffer_size: int     = 1e6 |> int      # Maximum size of replay buffer
α_start: float       = 0.6
β_start: float       = 0.4
β_frames: int        = 1e5 |> int

## NN Optimizer
w_init: float        = 3e-3            # Initial weight limit
η_π: float           = 3e-4            # Learning Rate for Actor
η_q: float           = 3e-4            # Learning Rate for Critic
η_α: float           = 3e-4            # Learning Rate for Critic
βs: tuple[float]     = (β1, β2) where: # Betas
    β1: float        = 0.9      # 0.9
    β2: float        = 0.999    # 0.999

## Setup
env_id: str     = f"gace:{ace_id}-{ace_backend}-v{ace_variant}"
time_stamp: str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
model_dir: str  = f"./models/{time_stamp}-{env_id}-{algorithm}"
model_path: str = f"{model_dir}/checkpoint.pt"
log_dir: str    = f"./runs/{time_stamp}-{env_id}-{algorithm}/"
os.makedirs(model_dir, exist_ok = True)

## Setup Globals
device = pt.device("cuda:1") if pt.cuda.is_available() else pt.device("cpu")
writer = SummaryWriter(log_dir = log_dir, flush_secs = 30)
_      = rng_seed |> pt.manual_seed

## Utility
def write_performance(env: gym.Env, step: int) = performance where:
    target      = env.target
    performance = env |> .ace |> ac.current_performance
    for k in target.keys():
        writer.add_scalars( k, { "performance": performance[k]
                               , "target": target[k] }
                          , step )

def limits_init(layer) = (-lim, lim) where:
    fan_in = layer.weight.data |> .size() |> .[0]
    lim = fan_in ** (-1/2)

def weights_init(layers, limits = None):
    if limits:
        for layer in layers:
            limits |*> layer.weight.data.uniform_
    else:
        for layer in layers:
            layer |> limits_init |*> layer.weight.data.uniform_

def soft_sync(source: pt.nn.Module, target: pt.nn.Module, τ: float = τ_soft):
    soften = (sp, tp) -> (tp.data * (1.0 - τ) + sp.data * τ) |> tp.data.copy_
    (source, target) |> map$(.parameters()) |*> zip |> starmap$(soften) |> list

def hard_sync(source: pt.nn.Module, target: pt.nn.Module):
    harden = (sp, tp) -> sp.data |> tp.data.copy_
    [source, target] |> map$(.parameters()) |*> zip |> starmap$(harden) |> list

def uniform_prior(a) = p where:
    p = a.shape[1] |> pt.zeros$(?, device = device) |> .reshape(-1,1) # |> to_device$(a)

def gaussian_prior(a) = p where:
    loc = a.shape[1] |> pt.zeros$(?, device = device) #|> to_device$(a)
    tri = a.shape[1] |> pt.eye$(?, device = device) # |> to_device$(a)
    p   = pt.distributions.MultivariateNormal(loc = loc, scale_tril = tri) \
        |> .log_prob(a) |> .reshape(-1,1) # |> to_device$(a)

## Data Processing
def process_gym(observations) = states where:
    states = [ obs |> pt.from_numpy for obs in observations
             ] |> pt.vstack |> .to(device)

def process_gace(observations, keys: dict[str, [list [str]]]) = states where:
    ok     = [ k for k in keys['observations'] 
               if (k[0].islower() or (k in keys['actions'])) 
                  and ("steps" not in k)
                  and not k.startswith('vn_')]
    idx    = [keys['observations'].index(m) for m in ok]
    idx_i  = [ok.index(i) for i in ok if i.startswith('i') or i.endswith(':id')]
    fl     = ["ugbw", "cof", "sr_f", "sr_r"]
    idx_f  = [ ok.index(f) for f in ok 
               if (not f.startswith('delta_') 
                   and (filter((f_) -> (f_ in f), fl) |> list |> any)) 
                  or f.endswith(':fug') ]
    msk_i  = [ i in idx_i for i in (ok |> len |> range) 
             ] |> pt.tensor$(?, dtype = pt.bool)
    msk_f  = [ f in idx_f for f in (ok |> len |> range) 
             ] |> pt.tensor$(?, dtype = pt.bool)
    obs    = observations |> map$(pt.from_numpy) |> list |> pt.vstack |> .[:,idx] 
    states = obs |> pt.where$(msk_f, (obs |> pt.abs |> pt.log), ?) \
                 |> pt.where$(msk_i, obs * 1e6, ?) |> pt.nan_to_num |> .to(device)

def scale_rewards(reward: pt.Tensor, ρ: float = 1e-3) = scaled_reward where:
    scaled_reward = (reward - pt.mean(reward)) / (pt.std(reward) + ρ)
 
## Replay Buffer
data ReplayBuffer( state: pt.Tensor, action: pt.Tensor, reward: pt.Tensor
                 , next_state: pt.Tensor, done: pt.Tensor ):
    def __len__(self) = self.state |> .shape |> .[0] |> int
    def __add__(self, other: ReplayBuffer) = buf where:
        buf = (self, other) |*> zip \
            |> map$(.[-buffer_size:] .. pt.cat) |*> ReplayBuffer

def sample_buffer(buffer: ReplayBuffer, batch_size: int) = smpl where:
    idx = buffer |> len |> pt.randperm |> .[:batch_size] |> .tolist()
    smpl = buffer |> map$(.[idx,:]) |*> ReplayBuffer

data PERBuffer( buffer: ReplayBuffer, priorities: pt.Tensor, α: float = α_start
              ,  β_start: float = β_start, β_frames: int = β_frames ):
    def __len__(self) = self.buffer |> len

def mk_per_buffer( α: float = 0.6,  β_start: float = 0.4, β_frames: int = int(1e5) 
                 ) = buffer where:
    buf    = pt.empty(0, device = device) |> repeat |> .$[:5] |> tuple |*> ReplayBuffer
    prio   = pt.empty(0, device = device)
    buffer = PERBuffer(buf, prio, α, β_start, β_frames)

def β_by_frame(buffer: PERBuffer, frame_idx: int) = β where:
    β = ( buffer.β_start + frame_idx * (1.0 - buffer.β_start) / buffer.β_frames
        ) |> min$(1.0,?)

def per_push( pb: PERBuffer,  state: pt.Tensor, action: pt.Tensor, reward: pt.Tensor
            , next_state: pt.Tensor, done: pt.Tensor ) = buffer where:
    b1, p1, α, β_start, β_frames = pb
    b2   = (state, action, reward, next_state, done) |*> ReplayBuffer
    buf  = b1 + b2
    p    = (if len(b1) > 0 
               then p1 |> pt.max |> .item() 
               else pt.tensor(1, device = device))
    prio = b2.reward |> pt.full_like$(?, p) |> (,)$(p1,?) |> pt.cat |> .[-buffer_size:]
    buffer = PERBuffer(buf, prio, α, β_start, β_frames)

def per_sample( buffer: PERBuffer, frame_idx: int, batch_size: int = batch_size
              ) = (s,i,w) where:
    N = buffer.buffer |> len
    p = buffer.priorities |> pt.pow$(?, buffer.α) |> .squeeze()
    P = p / p.sum()
    i = P.multinomial( num_samples = batch_size, replacement = False
                     ) |> pt.arange(0, N, device = device)$[]
    s = pt.index_select$(?,0,i) |> map$(?, buffer.buffer) |*> ReplayBuffer
    β = β_by_frame(buffer, frame_idx)
    W = (N * P[i]) |> pt.pow$(?,-β)
    w = W / W.max()

def per_update( buffer: PERBuffer, indices: pt.Tensor, priorities: pt.Tensor 
              ) = buffer_ where:
    b1, p1, α, β_start, β_frames = buffer
    p2   = p1 |> pt.index_select$(?, 0, indices) \
              |> pt.index_add$(p1, 0, indices, ?, alpha = -1) \
              |> pt.index_add$(?, 0, indices, priorities)
    buffer_ = PERBuffer(b1, p2, α, β_start, β_frames)

## Actor
#class Actor(pt.nn.Module)
data Actor(obs_dim: int, act_dim: int) from pt.nn.Module:
    def __init__( self, obs_dim: int, act_dim: int
                , σ_min: float = σ_min, σ_max: float = σ_max ):
        super(Actor, self).__init__()
        self.σ_min = σ_min
        self.σ_max = σ_max
        self.lin_1 = pt.nn.Linear(obs_dim, 256)
        self.lin_2 = pt.nn.Linear(256    , 256)
        self.lin_μ = pt.nn.Linear(256    , act_dim)
        self.lin_σ = pt.nn.Linear(256    , act_dim)
        self.activ = pt.nn.functional.relu
        weights_init([self.lin_1, self.lin_2])
        weights_init([self.lin_μ, self.lin_σ], limits = (-w_init, w_init))
    def forward(self, state) = (μ, σ) where:
        s = state |> self.lin_1 |> self.activ |> self.lin_2 |> self.activ
        μ = s |> self.lin_μ
        σ = s |> self.lin_σ |> pt.clamp$(?, self.σ_min, self.σ_max)

## Critic
data Critic(obs_dim: int, act_dim: int) from pt.nn.Module:
    def __init__(self, obs_dim: int, act_dim: int):
        super(Critic, self).__init__()
        dim           = obs_dim + act_dim
        self.lin_1 = pt.nn.Linear(dim, 256)
        self.lin_2 = pt.nn.Linear(256, 256)
        self.lin_3 = pt.nn.Linear(256, 1)
        self.activ = pt.nn.functional.relu
        weights_init([self.lin_1, self.lin_2])
        weights_init([self.lin_3])
    def forward(self, state, action) = q where:
        q = (state, action) |> pt.cat$(?, dim = 1) |> self.lin_1 |> self.activ \
                            |> self.lin_2 |> self.activ |> self.lin_3

## SAC Agent
data Agent( π_online: pt.nn.Module, π_optim: pt.optim.Optimizer
          , q1_online: pt.nn.Module, q1_optim: pt.optim.Optimizer
          , q2_online: pt.nn.Module, q2_optim: pt.optim.Optimizer
          , q1_target: pt.nn.Module, q2_target: pt.nn.Module
          , entropy_target: float, π_prior: Callable
          , α_log: pt.Tensor, α_optim: pt.optim.Optimizer ):
    def save_state(self, checkpoint_file: str) = res where:

        nets = [ self.π_online, self.q1_online, self.q2_online, self.q1_target
               , self.q2_target ]
        opts = [ self.π_optim, self.q1_optim, self.q2_optim , self.α_optim ]
        tens = [self.α_log]
        m    = ( nets |> map$(.state_dict() .. .cpu() .. .eval()) |> list
               ) + ( opts |> map$(.state_dict()) |> list
               ) + (tens |> map$(.cpu()) |> list)
        d    = [ "actor_online", "critic_1_online", "critic_2_online"
               , "critic_1_target", "critic_2_target", "actor_optim"
               , "critic_1_optim", "critic_2_optim", "alpha_optim", "alpha_log"]
        res  = (d,m) |*> zip |> dict |> pt.save$(?, checkpoint_file)
        _    = nets |> map$(.to(device) .. .train()) |> list
        _    = tens |> map$(.to(device)) |> list
    def load_state(self):
        raise(NotImplementedError)

def act(agent: Agent, state: pt.Tensor) = a where:
    μ,σ_log = state |> agent.π_online
    σ       = σ_log |> pt.exp
    N       = pt.distributions.Normal(0, 1)
    ε       = N |> .sample()
    a       = ( μ + σ * ε ) |> pt.tanh |> .detach()

def evaluate( agent: Agent, state: pt.Tensor
            , ε_n: float = ε_noise) = (a,p) where:
    μ,σ_log = state |> agent.π_online
    σ       = σ_log |> pt.exp
    N       = pt.distributions.Normal(0, 1)
    ε       = N |> .sample()
    a       = ( μ + σ * ε ) |> pt.tanh
    l1      = (μ, σ) |*> pt.distributions.Normal |> .log_prob( μ + σ * ε )
    l2      = (1 - pt.pow(a, 2) + ε_n) |> pt.log
    p       = l1 - l2

def make_agent( act_dim: int, obs_dim: int, prior: str = "Gaussian"
              ) = agent where:
    π_online       = Actor(obs_dim, act_dim)  |> .to(device)
    q1_online      = Critic(obs_dim, act_dim) |> .to(device)
    q1_target      = Critic(obs_dim, act_dim) |> .to(device)
    q2_online      = Critic(obs_dim, act_dim) |> .to(device)
    q2_target      = Critic(obs_dim, act_dim) |> .to(device)
    _              = q1_online.state_dict()   |> q1_target.load_state_dict
    _              = q2_online.state_dict()   |> q2_target.load_state_dict
    π_optim        = pt.optim.Adam( π_online.parameters()
                                  , lr = η_π, betas = βs )
    q1_optim       = pt.optim.Adam( q1_online.parameters()
                                  , lr = η_q, betas = βs )
    q2_optim       = pt.optim.Adam( q2_online.parameters()
                                  , lr = η_q, betas = βs )
    entropy_target = - act_dim
    α_log          = pt.tensor([0.0], requires_grad = True, device = device)
    α_optim        = pt.optim.Adam([α_log], lr = η_α, betas = βs)
    π_prior        = .to(device) .. (if prior == "Uniform"
                                        then uniform_prior
                                        else gaussian_prior)
    agent          = Agent( π_online, π_optim, q1_online, q1_optim
                          , q2_online, q2_optim, q1_target, q2_target
                          , entropy_target, π_prior, α_log, α_optim)

def update_step(iteration, 0, agent, experiences, _, prios, _, _) = prios where:
    if verbose and (iteration in count(0, 10)):
        _ = print(f"Finished Updating after {iteration:05} iterations.")
addpattern def update_step( iteration, epoch, agent, experiences, weights
                          , prios = None, γ = 0.99, d = 1 
                          ) = update_step( iteration, epoch_, agent, experiences
                                         , weights, prios_, γ, d ) where:
    states, actions, rewards_, next_states, dones = experiences
    next_actions, next_logprobs_                  = evaluate agent next_states
    rewards        = rewards_ * reward_scale
    #rewards        = rewards_ |> scale_rewards
    next_logprobs  = next_logprobs_ |> pt.mean
    next_q1_target = (next_states, next_actions) |*> agent.q1_target
    next_q2_target = (next_states, next_actions) |*> agent.q2_target
    next_q_target  = (next_q1_target, next_q2_target) |*> pt.min
    α_             = agent.α_log |> pt.exp
    q_targets      = rewards \
                   + (γ * (1 - dones) * (next_q_target - α_ * next_logprobs)) \
                   |> .detach() |> pt.clone
    now_q1         = (states, actions) |*> agent.q1_online
    now_q2         = (states, actions) |*> agent.q2_online
    td1_error      = q_targets |> (-)$(?,now_q1)
    td2_error      = q_targets |> (-)$(?,now_q2)
    q1_loss        = td1_error |> pt.pow$(?, 2) |> (*)$(?, weights) \
                   |> pt.mean |> (*)$(0.5,?) |> pt.clone
    q2_loss        = td2_error |> pt.pow$(?, 2) |> (*)$(?, weights) \
                   |> pt.mean |> (*)$(0.5,?) |> pt.clone
    _              = agent.q1_optim.zero_grad(set_to_none = True)
    _              = q1_loss.backward()
    _              = agent.q1_optim.step()
    _              = agent.q2_optim.zero_grad(set_to_none = True)
    _              = q2_loss.backward()
    _              = agent.q2_optim.step()
    if iteration in count(0, d):
        actions_, logprobs_                       = evaluate agent states
        logprobs   = logprobs_ |> .detach() |> pt.clone
        α_loss     = - ( agent.α_log * (logprobs + agent.entropy_target)
                       ) |> pt.mean
        _          = agent.α_optim.zero_grad(set_to_none = True)
        _          = α_loss.backward()
        _          = agent.α_optim.step()
        α          = agent.α_log |> pt.exp |> .item()
        π_pl       = agent.π_prior actions_
        with pt.no_grad():
            q1_    = agent.q1_online states actions_
        π_loss     = weights |> .reshape(-1,1) \
                   |> (*)$((α * logprobs_ - q1_ - π_pl), ?) |> pt.mean
        _          = agent.π_optim.zero_grad(set_to_none = True)
        _          = π_loss.backward()
        _          = agent.π_optim.step()
        _          = soft_sync(agent.q1_online, agent.q1_target)
        _          = soft_sync(agent.q2_online, agent.q2_target)
        _          = writer.add_scalar("_Loss_π", π_loss, iteration)
        _          = writer.add_scalar("_Loss_α", α_loss, iteration)
    prios_         = (0.5 * (td1_error + td2_error) + 1e-5) |> pt.abs |> .detach()
    epoch_         = epoch - 1
    _              = writer.add_scalar("_Loss_Q1", q1_loss, iteration)
    _              = writer.add_scalar("_Loss_Q2", q2_loss, iteration)

def update_policy( iteration: int, agent: Agent, buffer: PERBuffer
                 , num_epochs: int = num_epochs ) = buffer_ where:
    experiences, indices, weights = per_sample(buffer, iteration, batch_size)
    prios = update_step(iteration, num_epochs, agent, experiences, weights)
    buffer_ = per_update(buffer, indices, prios)

## Evaluate Policy
def evaluate_step( iteration, 0, _, envs, buffer, states, iter_total
                 ) = (buffer, states, iter_total) where:
    if verbose and (iteration in count(0, 10)):
        r = iter_total |> pt.sum$(?, dim = 0) |> pt.mean |> .item()
        _ = print(f"Finished Evauluating after {iteration:05} iterations with Total Reward: {r:.3f}")
addpattern def evaluate_step( iteration, step, agent, envs, buffer, states, iter_total
                            ) = evaluate_step( iteration, step_, agent, envs
                                             , buffer_, states_ , iter_total_
                                             )  where:
    with pt.no_grad():
        actions = act agent states 
    t0          = time.time()
    observations,rewards_,dones_,infos \
                = actions |> pt.split$(?,1) \
                          |> fmap$(.numpy() .. .cpu() .. .squeeze()) \
                          |> list |> envs.step
    t1          = time.time()
    dt          = t1 - t0
    keys       = envs.info[0]
    states_     = process_gace observations keys
    #states_     = process_gym observations
    rewards     = rewards_ |> pt.tensor$(?, device = device, dtype = pt.float) \
                           |> .reshape(-1,1)
    dones       = dones_   |> pt.tensor$(?, device = device, dtype = pt.float) \
                           |> .reshape(-1,1)
    buffer_     = buffer + ReplayBuffer(states, actions, rewards, states_, dones)
    step_       = step - 1
    iter_total_ = rewards_ |> pt.tensor |> .reshape(1,-1) |> (,)$(iter_total,?) |> pt.cat
    _           = writer.add_scalar("_Reward_Mean", rewards |> pt.mean, iteration)
    _           = envs$[0] |> write_performance$(?, iteration)

def evaluate_policy( iteration: int, agent: Agent, envs: gace.envs.vec.VecACE
                   , buffer: ReplayBuffer, states: pt.Tensor
                   , num_steps: int = num_steps 
                   ) = (buffer_, states_, reward_) where:
    reward                  = pt.empty(0)
    buffer_,states_,reward_ = evaluate_step( iteration, num_steps, agent, envs
                                           , buffer, states, reward )

## Run Loop
def run_algorithm( episode, iteration, agent, envs, True, buffer, _, reward
                 ) = reward where:
    tm = reward |> pt.sum$(?, dim = 0) |> pt.mean |> .item()
    _  = writer.add_scalar(f"_Reward_Total_Mean", tm, episode)
    ti = reward |> pt.sum$(?, dim = 0) |> pt.min  |> .item()
    _  = writer.add_scalar(f"_Reward_Total_Min",  ti, episode)
    ta = reward |> pt.sum$(?, dim = 0) |> pt.max  |> .item()
    _  = writer.add_scalar(f"_Reward_Total_Max",  ta, episode)
    if verbose:
        print(f"Episode {episode:03} done after {iteration:05} iterations with Min Reward: {ti}")
    _  = model_path |> agent.save_state
addpattern def run_algorithm( episode, iteration, agent, envs, done, buffer
                            , states, reward 
                            ) = run_algorithm( episode, iteration_, agent, envs
                                             , done_, buffer_, states_, reward_
                                             ) where:
    memories,_states,eval_reward \
               = evaluate_policy( iteration, agent, envs, buffer.buffer, states
                                , num_steps )
    reward_    = pt.cat((reward, eval_reward))
    buffer_    = memories |*> per_push$(buffer) \
               |> (if len(memories) < batch_size
                      then reveal_type 
                      else update_policy$(iteration, agent, ?, num_epochs))
    iteration_ = iteration + 1
    dones      = - (envs.num_envs * num_steps) |> int |> slice$(?, None) \
               |> memories.done$[] |> pt.squeeze |> .to(pt.bool) |> .tolist()
    states_    = if any(dones) then envs.reset(done_mask = dones) else _states
    #states_    = if any(dones) then envs.reset() else _states
    done_      = (iteration >= num_iterations) or all(dones)
    _          = model_path |> agent.save_state

def run_episodes( agent: Agent, envs: gace.envs.vec.VecACE, episode: int
                ) = agent where:
    obs    = envs.reset()
    keys   = envs.info[0]
    states = process_gace obs keys
    #states = process_gym obs
    buffer = mk_per_buffer()
    reward = pt.empty(0)
    total  = run_algorithm(episode, 0, agent, envs, False, buffer, states, reward)
