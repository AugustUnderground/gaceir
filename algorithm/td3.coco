import coconut.convenience

import os, time, datetime
from collections import namedtuple
from itertools import repeat
import torch as pt
from torch.utils.data import TensorDataset, DataLoader
import torch_optimizer as optim
from torch.utils.tensorboard import SummaryWriter
from fastprogress.fastprogress import master_bar, progress_bar
import gym, gace
import hace as ac
from hy.core import *
from hy.core.language import *
from hy.contrib.pprint import pp

## Defaults (as args later)
verbose: bool        = True            # Print verbose debug output
num_episodes: int    = 42              # Number of episodes to play
num_steps: int       = 2               # How many steps to take
num_iters: int       = 100             # Iterations per Epoch
num_epochs: int      = 100             # How many time steps to update policy
early_stop: float    = -50.0           # Early stop criterion
batch_size: int      = 100             # size of the batches during epoch
act_std: float       = 0.25            # standard deviation action distribution
rng_seed: int        = 666             # Random seed for reproducability
algorithm: str       = "td3"           # Name of used algorithm ∈  ./algorithm
ace_id: str          = "op2"           # ACE Identifier of the Environment
ace_backend: str     = "xh035"         # PDK/Technology backend of the ACE Environment
ace_variant: int     = 0               # ACE Environment variant
γ: float             = 0.99            # Discount Factor
τ_soft: float        = 1e-2            # Avantage Factor
α: float             = 1e-4            # Learning Rate
βs: tuple[float]     = (β1, β2) where: # Weight Decay
    β1: float        = 0.9
    β2: float        = 0.999
exp_noise: float     = 0.15            # Exploration Noise
pol_noise: float     = 0.2             # Noise added to replay actions
clp_noise: float     = 0.5             # Noise clipping
update_interval: int = 2               # Update every n epochs
buffer_size: int     = 1e7 |> int      # Maximum size of replay buffer
warmup_periode: int  = 100             # Collect experience during warmup

## Setup
env_id: str     = f"gace:{ace_id}-{ace_backend}-v{ace_variant}"
time_stamp: str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
model_dir: str  = f"./models/{time_stamp}-{env_id}-{algorithm}"
model_path: str = f"{model_dir}/checkpoint.pt"
log_dir: str    = f"./runs/{time_stamp}-{env_id}-{algorithm}/"
os.makedirs(model_dir, exist_ok = True)

## Setup Globals
device = pt.device("cuda:1") if pt.cuda.is_available() else pt.device("cpu")
writer = SummaryWriter(log_dir = log_dir, flush_secs = 30)
_      = rng_seed |> pt.manual_seed

## Utility
def write_performance(env: gym.Env, step: int) = performance where:
    target      = env.target
    performance = env |> .ace |> ac.current_performance
    for k in target.keys():
        writer.add_scalars( k, { "performance": performance[k]
                               , "target": target[k] }
                          , step )

## Replay Buffer
data ReplayBuffer( state: pt.Tensor, action: pt.Tensor, reward: pt.Tensor
                 , next_state: pt.Tensor, done: pt.Tensor ):
    def push(self, other) = new where:
        new = (if len(self.state) <= 0 
                  then other |*> ReplayBuffer |> fmap$(.[-buffer_size:])
                  else zip(self,other) |> fmap$(.[-buffer_size:] .. pt.vstack) |*> ReplayBuffer)
    def sample(self, batch_size: int) = smpl where:
        idx = self.state |> .shape |> .[0] |> pt.randperm |> .[:batch_size] |> .tolist()
        smpl = (e[idx,:] for e in self) |*> ReplayBuffer
    def size(self) = self.state |> .shape |> .[0] |> float

## Neural Networks
data HStack() from pt.nn.Module:
    def forward(self, X: tuple[pt.Tensor, pt.Tensor]) = X |> pt.hstack

data VStack() from pt.nn.Module:
    def forward(self, X: tuple[pt.Tensor, pt.Tensor]) = X |> pt.vstack

## Critic
def critic_net(obs_dim: int, act_dim: int) = crt_net where:
    dim = obs_dim + act_dim
    crt_net = pt.nn.Sequential( HStack()
                              , pt.nn.Linear(dim, 256), pt.nn.ReLU()
                              , pt.nn.Linear(256, 128), pt.nn.ReLU()
                              , pt.nn.Linear(128, 64) , pt.nn.ReLU()
                              , pt.nn.Linear(64 , 1)  )

## Actor
def actor_net(obs_dim: int, act_dim: int) = act_net where:
    act_net = pt.nn.Sequential( pt.nn.Linear(obs_dim, 128)    , pt.nn.ReLU()
                              , pt.nn.Linear(128    , 256)    , pt.nn.ReLU()
                              , pt.nn.Linear(256    , 128)    , pt.nn.ReLU()
                              , pt.nn.Linear(128    , 64)     , pt.nn.ReLU()
                              , pt.nn.Linear(64     , act_dim), pt.nn.Tanh())

def soft_update( source: pt.nn.Module, target: pt.nn.Module, τ: float = 1e-2
               ) = target where:
    for param, target_param in zip(source.parameters(), target.parameters()):
        target_param.data.copy_(τ * param.data + (1 - τ) * target_param.data)

data Model( Q1: pt.nn.Module, Q2: pt.nn.Module, π: pt.nn.Module
          , θ1: pt.nn.Module, θ2: pt.nn.Module, φ: pt.nn.Module 
          , Q1_optim: pt.optim.Optimizer, Q2_optim: pt.optim.Optimizer
          , π_optim: pt.optim.Optimizer, Q_loss ):
    def select_action(self, state: pt.Tensor, step: int) = action where:
        with pt.no_grad():
            action = state |> self.π |> add_noise$(?,t = step) |> .detach()
        if verbose:
            print(f"Selecting Agent Action.")
    def eval_mode(self) = self where:
        [ self.π, self.Q1, self.Q2, self.φ, self.θ1, self.θ2
        ] |> fmap$(.to(device) .. .eval())
    def train_mode(self) = self where:
        [ self.π, self.Q1, self.Q2, self.φ, self.θ1, self.θ2
        ] |> fmap$(.to(device) .. .train())

def make_model(act_dim: int, obs_dim: int) = model where:
    Q1       = critic_net(obs_dim, act_dim) |> .to(device)
    Q2       = critic_net(obs_dim, act_dim) |> .to(device)
    π        = actor_net(obs_dim, act_dim)  |> .to(device)
    θ1       = critic_net(obs_dim, act_dim) |> .to(device)
    θ2       = critic_net(obs_dim, act_dim) |> .to(device)
    φ        = actor_net(obs_dim, act_dim)  |> .to(device)
    _        = soft_update(Q1, θ1, τ = 1.0)
    _        = soft_update(Q2, θ2, τ = 1.0)
    _        = soft_update(π , φ , τ = 1.0)
    Q1_optim = pt.optim.Adam(Q1.parameters(), lr = α, betas = βs)
    Q2_optim = pt.optim.Adam(Q2.parameters(), lr = α, betas = βs)
    π_optim  = pt.optim.Adam(π.parameters() , lr = α, betas = βs)
    Q_loss   = pt.nn.MSELoss()
    model    = Model(Q1, Q2, π, θ1, θ2, φ, Q1_optim, Q2_optim, π_optim, Q_loss)

## Exploitation
def add_noise( a, t = 0, lo = -1.0, hi = 1.0, σ_max = 1.0, σ_min = 1.0, d = 1000000
             ) = a_ where:
    σ  = σ_max - (σ_max - σ_min) * min(1.0, t / d)
    a_ = (a + pt.randn_like(a) * σ) |> pt.clip$(?, min = lo, max = hi)

def random_action(envs) = action where:
    action = envs.action_space |> fmap$(pt.from_numpy .. .sample()) \
                               |> pt.vstack |> .detach() |> .to(device)
    if verbose:
        print(f"WARMUP: Selecting Random Action.")

def policy_update(epoch, model, states) = π_loss where:
    π_loss = states |> model.π |> (,)$(states,?) |> model.Q1 |> .mean() |> (-)
    _      = model.π_optim.zero_grad()
    _      = π_loss.backward()
    _      = model.π_optim.step()
    _      = soft_update(model.Q1, model.θ1, τ = τ_soft)
    _      = soft_update(model.Q2, model.θ2, τ = τ_soft)
    _      = soft_update(model.π , model.φ , τ = τ_soft)
    _      = writer.add_scalar("_Loss_π" , π_loss , epoch)

def update(epoch, model, buffer) = (q1_loss, q2_loss, π_loss) where:
    states, actions, rewards, next_states, dones \
                 = buffer.sample(batch_size) |> fmap$(.to(device))
    noise        = actions |> pt.zeros_like |> pt.normal$(?,pol_noise) \
                           |> pt.clamp$(?,-clp_noise, clp_noise) |> .to(device)
    next_actions = states |> model.φ |> (+)$(?,noise) |> pt.clamp$(?, -1.0, 1.0)
    q1_target    = (next_states, next_actions) |> model.θ1
    q2_target    = (next_states, next_actions) |> model.θ2
    q_target     = pt.min(q1_target, q2_target)
    q_expected   = (rewards + (1.0 - dones) * γ * q_target) |> .detach()
    q1           = (states, actions) |> model.Q1
    q2           = (states, actions) |> model.Q2
    q1_loss      = model.Q_loss(q1, q_expected)
    q2_loss      = model.Q_loss(q2, q_expected)
    _            = model.Q1_optim.zero_grad()
    _            = q1_loss.backward()
    _            = model.Q1_optim.step()
    _            = model.Q2_optim.zero_grad()
    _            = q2_loss.backward()
    _            = model.Q2_optim.step()
    π_loss       = (if epoch % 2 == 0 # epoch in count(0, update_interval) 
                       then policy_update(epoch, model, states)
                       else pt.nan |> pt.tensor |> .to(device))
    _            = writer.add_scalar("_Loss_Q1", q1_loss, epoch)
    _            = writer.add_scalar("_Loss_Q2", q2_loss, epoch)
    #if verbose:
    #    print(f"Epoch {epoch:03} | Q Loss: {q1_loss:3f}, {q2_loss:3f}")

## Exploration
def postprocess(observations, keys) = states where:
    idx = [ key |> filter$((k) -> ":" not in k, ?) |> map$((p) -> p |> key.index) |> list
            for key in keys ] 
    states = [ o[i] for o,i in zip(observations, idx)
             ] |> fmap$(pt.from_numpy) |> pt.vstack |> .to(device)

def explore( envs, model, step, states, buffer
           ) = (next_states, new_buffer) where:
    actions     = (if (step * len(envs)) < warmup_periode
                      then random_action(envs)
                      else model.select_action(states, step))
    t0          = time.time()
    observations,rewards_,dones_,infos \
                = actions |> pt.split$(?,1) \
                          |> fmap$(.numpy() .. .cpu() .. .squeeze()) \
                          |> list |> envs.step
    next_states = postprocess(observations, [i["output-parameters"] for i in infos])
    rewards     = rewards_ |> pt.Tensor |> .to(device) |> .reshape(-1,1)
    dones       = dones_ |> pt.Tensor |> .to(device) |> .reshape(-1,1)
    new_buffer  = (states, actions, rewards, next_states, dones) \
                |*> ReplayBuffer |> buffer.push
    t1          = time.time()
    _           = writer.add_scalar("_Reward_Mean", rewards.mean().item(), step)
    if verbose:
        print(f"Step {step:03} took {(t1 - t0):.3f}s  | Average Reward: {rewards.mean():.3f}")

## Checkpoint Saving
def save_checkpoint(model: Model, checkpoint_file: str) = res where:
    state_dicts = [ model.Q1, model.Q2, model.π, model.θ1, model.θ2, model.φ
                  , model.Q1_optim, model.Q2_optim ] |> fmap$(.state_dict())
    keys        = [ "Q1", "Q2", "pi", "T1", "T2", "phi"
                  , "Q1_opt", "Q2_opt", "pi_opt" ]
    save_dict   = (keys, state_dicts) |*> zip |> dict
    res         = pt.save(save_dict, checkpoint_file)

## Run Episode until done
def run_episode(_, model, episode, _, _, buffer, True) = model where:
    total   = buffer.reward |> .sum() |> .item()
    _       = writer.add_scalar(f"_Reward_Total", total, episode)
    if verbose:
        f"Episode {episode:03} Finished | Total Reward: {total}" |> print
addpattern def run_episode( envs, model, episode, iteration, states, buffer, done
                          ) = run_episode( envs, model, episode, iteration + 1
                                         , next_states, new_buffer, finish 
                                         ) where:
    _      = model.eval_mode()
    expl   = (sb, e) -> sb |*> explore$(envs, model, e, ?)
    steps  = range(iteration, iteration + num_steps)
    next_states, new_buffer \
           = reduce(expl, steps, (states, buffer))
    done_  = .item() .. pt.all .. .bool() .. .squeeze() .. .done <| new_buffer
    last   = len(envs) + num_steps
    stop_  = new_buffer.reward[-last:].mean().item() < early_stop
    finish = done_ or stop_ or (iteration >= num_iters)
    _      = model.train_mode()
    q1_loss, q2_loss, π_loss_ \
           = [ update(epoch, model, new_buffer) 
               for epoch in num_epochs |> range 
             ]  |*> zip |> fmap$pt.hstack |> tuple
    π_loss = π_loss_[~π_loss_.isnan()]
    _      = envs$[0] |> write_performance$(?, iteration)

def run_episodes( model: Model, envs: gace.envs.vec.VecACE, episode: int
                ) = model where:
    obs     = envs.reset()
    states  = postprocess(obs, envs.observation_keys)
    buffer  = pt.empty(0) |> repeat |> .$[:5] |> tuple |*> ReplayBuffer
    memory  = run_episode(envs, model, episode, 0, states, buffer, False)
    _       = model.eval_mode()
    _       = save_checkpoint(model, model_path)

