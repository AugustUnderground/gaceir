import coconut.convenience

import os, time, datetime
import torch as pt
from torch.utils.data import TensorDataset, DataLoader
import torch_optimizer as optim
from torch.utils.tensorboard import SummaryWriter
from fastprogress.fastprogress import master_bar, progress_bar
import gym, gace
import hace as ac

## Defaults (as args later)
verbose: bool     = True            # Print verbose debug output
#num_envs: int     = os.sched_getaffinity(0) |> len |> (//)$(?,2)
num_envs: int     = 50
num_episodes: int = 42              # Number of episodes to play
num_steps: int    = 25              # How many steps to take -> num_steps × num_envs = n_points ∈  data_set
num_epochs: int   = 100             # How many time steps to update policy
early_stop: float = -50.0           # Early stop criterion
batch_size: int   = 100             # size of the batches during epoch
act_std: float    = 0.25            # standard deviation action distribution
rng_seed: int     = 666             # Random seed for reproducability
algorithm: str    = "ppo"           # Name of used algorithm ∈  ./algorithm
ace_id: str       = "op2"           # ACE Identifier of the Environment
ace_backend: str  = "xh035"         # PDK/Technology backend of the ACE Environment
ace_variant: int  = 0               # ACE Environment variant
ε: float          = 0.2             # Factor for clipping
η: float          = 0.001           # Factor in loss function
γ: float          = 0.99            # Discount Factor
τ: float          = 0.95            # Avantage Factor
α: float          = 1e-3            # Learning Rate
βs: tuple[float]  = (β1, β2) where: # Weight Decay
    β1: float     = 0.9
    β2: float     = 0.999

## Setup
env_id: str     = f"gace:{ace_id}-{ace_backend}-v{ace_variant}"
time_stamp: str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
model_dir: str  = f"./models/{time_stamp}-{env_id}-{algorithm}"
model_path: str = f"{model_dir}/checkpoint.pt"
log_dir: str    = f"./runs/{time_stamp}-{env_id}-{algorithm}/"
os.makedirs(model_dir, exist_ok = True)

## Setup Globals
device = pt.device("cuda:1") if pt.cuda.is_available() else pt.device("cpu")
writer = SummaryWriter(log_dir = log_dir, flush_secs = 30)
_      = rng_seed |> pt.manual_seed

## Environment setup
envs: gace.envs.vec.VecACE = gace.vector_make_same(env_id, num_envs)
#obs_dim: int = envs.observation_space[0].shape[0]
obs_dim: int = envs$[0].target |> len |> (*)$(3)
#obs_dim: int = envs$[0].target |> len |> (*)$(2)
act_dim: int = envs.action_space[0].shape[0]

## Network Generators for ACE Environments
def act_net(obs_dim: int, act_dim: int) = net where:
    net = pt.nn.Sequential( pt.nn.Linear(obs_dim, 128)    , pt.nn.Tanh()
                          , pt.nn.Linear(128    , 256)    , pt.nn.Tanh()
                          , pt.nn.Linear(256    , 128)    , pt.nn.Tanh()
                          , pt.nn.Linear(128    , 64)     , pt.nn.Tanh()
                          , pt.nn.Linear(64     , act_dim), pt.nn.Tanh() )

def crt_net(obs_dim: int) = net where:
    net = pt.nn.Sequential( pt.nn.Linear(obs_dim, 128), pt.nn.Tanh()
                          , pt.nn.Linear(128    , 256), pt.nn.Tanh()
                          , pt.nn.Linear(256    , 128), pt.nn.Tanh()
                          , pt.nn.Linear(128    , 64) , pt.nn.Tanh()
                          , pt.nn.Linear(64     , 1)  )

data Memory( states: pt.Tensor, actions: pt.Tensor, logprobs: pt.Tensor
           , rewards: pt.Tensor, values: pt.Tensor, masks: pt.Tensor ):
    def __add__(self, other) = (self, other) |*> zip |> fmap$(pt.cat) |*> Memory
    def data_loader():
        states,actions,logprobs,rewards,values,masks \
                   = self |> fmap$(.[:-1]) |*> Memory
        values_    = self.values[1:]
        returns    = gae(rewards, values, masks, values_)
        advantages = returns - values
        loader     = (states, actions, logprobs, returns, advantages) \
                   |*> TensorDataset \
                   |> DataLoader$(?, batch_size = batch_size, shuffle = True)

def std_deviation(act_dim: int, act_std: float = 0.0) = std where:
    std = act_dim |> pt.ones$(1,?) |> (*)$(?,act_std)

def continuous_actor_critic( actor: pt.nn.Module, critic: pt.nn.Module
                           , std: float, states: pt.Tensor 
                           ) = (π, v) where:
    μ = actor(states)
    #σ = std |> .expand_as(μ) |> pt.diag_embed |> .to(device)
    σ = μ |> .shape |> .[1] |> pt.eye |> (*)$(?,std) |> .to(device)
    π = pt.distributions.MultivariateNormal(μ, σ)
    v = critic(states) |> .squeeze()

def discrete_actor_critic(actor, critic, states) = (π, v) where:
    μ = actor(states)
    π = pt.distributions.Categorical(μ)
    v = critic(states) |> .squeeze()

def update_policy( actor_critic: function, optimizer: pt.optim
                 , states: pt.Tensor, actions: pt.Tensor, logprobs: pt.Tensor
                 , returns:pt.Tensor, advantages: pt.Tensor 
                 ) = losses.detach() where:
    dist,values = actor_critic(states)
    entropy     = dist |> .entropy() |> .mean()
    logprobs_   = dist.log_prob(actions)
    ratios      = (logprobs_ - logprobs) |> pt.exp
    surr_1      = ratios * advantages
    surr_2      = pt.clamp(ratios, 1.0 - ε, 1.0 + ε) * advantages
    loss_act    = (surr_1, surr_2) |*> pt.min |> (-)
    loss_crt    = (returns - values) |> pt.pow$(?,2) |> pt.mean
    losses      = 0.5 * loss_crt + loss_act - η * entropy
    _           = optimizer         |> .zero_grad()
    _           = losses |> pt.mean |> .backward()
    _           = optimizer         |> .step()

def subset(obs, inf, pre) = sub where:
    sub = [ [ i["output-parameters"].index(p)
              for p in i["output-parameters"] if p.startswith(pre)
            ] for i in inf 
          ] |> zip$(?,obs) |> starmap$(pt.from_numpy .. ((i,o) -> o[i])) \
            |> list |> pt.vstack |> .to(device) |> .detach()

def postprocess(observations, infos) = states where:
    #performances = subset(observations, infos, "performance") \
    #             / subset(observations, infos, "target")
    #distances    = subset(observations, infos, "distance")
    #states       = (performances, distances) |> pt.hstack
    states = [ [ i["output-parameters"].index(p) for p in i["output-parameters"] 
                 if ["performance", "target", "distance"] |> fmap$(p.startswith, ?) |> any 
               ] for i in infos 
             ] |> zip$(?,observations) |> starmap$(pt.from_numpy .. ((i,o) -> o[i])) \
               |> list |> pt.vstack |> .to(device) |> .detach()
    #states = observations |> fmap$pt.from_numpy |> pt.vstack |> .to(device) |> .detach() 

def trajectories( actor_critic: function, envs: gace.envs.vec.VecACE
                , states: pt.Tensor ) = (memories, next_states) where:
    dist,values = actor_critic(states)
    actions     = dist |> .sample() |> .detach()
    logprobs    = actions |> dist.log_prob
    observations,rewards_,dones,infos \
                = actions |> pt.split$(?,1) \
                          |> fmap$(.numpy() .. .cpu() .. .squeeze()) \
                          |> list |> envs.step
    next_states = postprocess(observations, infos)
    masks       = dones |> pt.Tensor |> .to(device) |> (-)$(1,?)
    rewards     = rewards_ |> pt.Tensor |> .to(device)
    memories    = Memory(states,actions,logprobs,rewards,values,masks)

def gae( r: pt.Tensor, v: pt.Tensor, m: pt.Tensor, v_: pt.Tensor
       , γ: float = 0.99, τ: float = 0.95 ) = a where:
    δ = r + γ * v_ * m - v
    l = δ |> .shape |> .[0] |> range |> reversed
    i = pt.Tensor([0]).to(device)
    g = reduce( (g_,i_) -> δ[i_] + γ * τ * m[i_] * g_[0] |> (,)$(?,g_) |> pt.hstack
              , l, i ) |> .[:-1]
    a = v + g

def save_checkpoint( actor: pt.nn.Module, critic: pt.nn.Module
                   , optimizer: pt.optim, checkpoint_file: str
                   ) = res where:
    state_dicts = [actor, critic, optimizer] |> fmap$(.state_dict())
    save_dict   = state_dicts |> (,)$(["actor", "critic", "optimizer"],?) \
                |*> zip |> dict
    res = pt.save(save_dict, checkpoint_file)

def write_performance(env: gym.Env, step: int, episode: int) = performance where:
    target      = env.target
    performance = env |> .ace |> ac.current_performance
    for k in target.keys():
        writer.add_scalars( k, { "Episode_{episode}": performance[k] }
                          , step )

def run_episode(_, _, True, loss, reward) = (loss, reward)
addpattern def run_episode( episode, states, _, loss, reward
                          ) = run_episode( episode, next_states, done, loss_, reward_  
                                         )  where:
    def traject((memories,states), step) = (memories_, states_) where:
        t0             = time.time()
        memory,states_ = trajectories(actor_critic, envs, states)
        memories_      = Memory + memories
        t1             = time.time()
        if verbose:
            print(f"Step {step:03}/{num_steps} took {(t1 - t0):.3f}s | Average Reward: {memory.reward.mean():.3f}")
        writer.add_scalars("_Mean_Step_Reward", {f"Episode_{episode}": memory.reward.mean()}, step)
        _              = envs |> .gace_envs |> .[0] |> write_performance$(?, step, episode)
    def train(epoch, loader) = loss |> pt.mean where:
        t0   = time.time()
        loss = starmap(update, loader) |> list |> pt.stack
        t1   = time.time()
        if verbose:
            print(f"Epoch {epoch:03}/{num_epochs} took {(t1 - t0):.3f}s | Average Loss: {loss.mean():.3f}")
        writer.add_scalars("_Mean_Epoch_Loss", {f"Episode_{episode}": loss.mean().item()}, epoch)
        writer.add_scalars("_Sum_Epoch_Loss", {f"Episode_{episode}": loss.sum().item()}, epoch)
    _          = policy.eval()
    with pt.no_grad(): 
        memories_,next_states \
               = reduce(traject, range(num_steps), ([], states))
    _          = policy.train()
    loader     = data_loader(memories_)
    losses     = [ train(epoch, loader) for epoch in num_epochs |> range 
                 ] |> pt.stack |> .detach()
    done_      = .item() .. pt.all .. .bool() <| (1 - masks)
    stop_      = rewards.mean().item() < early_stop
    done       = done_ or stop_
    loss_      = pt.cat((loss, losses))
    reward_    = pt.cat((reward, rewards))
    if verbose:
        if done:
            print(f"All Environments Done")
        else:
            print(f"Episode {episode:03}/{num_episodes} | Average Reward {rewards.mean().item():.3f}")
    _          = save_checkpoint(actor, critic, optimizer)

def run_episodes(episode: int) = (act,crt) where:
    if verbose:
        print(f"Starting Episode {episode:03}")
    states = envs.reset() |> fmap$(.[:obs_dim] .. pt.from_numpy) |> pt.vstack |> .to(device)
    l = r = pt.empty(0) |> .to(device = device)
    losses,rewards = run_episode(episode, states, False, l, r)
    writer.add_scalar(f"_Mean_Epsiode_Loss", losses.mean().item(), episode)
    writer.add_scalar(f"_Total_Episode_Reward", rewards.sum().item(), episode)
    for i in losses |> len |> range:
        writer.add_scalars(f"Epsiode_Loss", {f"Episode_{episode}": losses[i]}, i)
        writer.add_scalars(f"Epsiode_Reward", {f"Episode_{episode}": rewards[i]}, i)
    if verbose:
        print(f"Episode {episode:03} | Total Reward: {rewards.sum():.3f}")
        print(f"Episode {episode:03} | Average Loss: {losses.mean():.3f}")
        print(f"Finished Episode {episode:03}")
    act = .cpu() .. .eval() <| actor
    crt = .cpu() .. .eval() <| critic

## Actor Critic Model
actor: pt.nn.Module        = act_net(obs_dim, act_dim) |> .to(device)
critic: pt.nn.Module       = crt_net(obs_dim) |> .to(device)
#deviation: pt.nn.Parameter = std_deviation(act_dim, act_std) |> .to(device)
policy: pt.nn.ModuleList   = pt.nn.ModuleList([actor, critic])

actor_critic: function     = continuous_actor_critic$(actor, critic, act_std)
#actor_critic: function     = continuous_actor_critic$(actor, critic, deviation)
#actor_critic: function     = discrete_actor_critic$(actor, critic)

optimizer: pt.optim        = pt.optim.Adam(policy.parameters(), lr = α, betas = βs)
#actor_optim: pt.optim      = pt.optim.Adam(actor.parameters(), lr = α, betas = βs)
#critic_optim: pt.optim     = pt.optim.Adam(critic.parameters(), lr = α, betas = βs)

#update: function           = update_policy$(actor_critic, actor_optim, critic_optim)
update: function           = update_policy$(actor_critic, optimizer)
