import coconut.convenience

import os, time, datetime
from itertools import repeat
import torch as pt
from torch.utils.data import TensorDataset, DataLoader
import torch_optimizer as optim
from torch.utils.tensorboard import SummaryWriter
from fastprogress.fastprogress import master_bar, progress_bar
import gym, gace
import hace as ac
from hy.core import *
from hy.core.language import *
from hy.contrib.pprint import pp

## Defaults (as args later)
verbose: bool     = True            # Print verbose debug output
num_episodes: int = 666             # Number of episodes to play
num_steps: int    = 5               # How many steps to take -> num_steps × num_envs = n_points ∈  data_set
num_epochs: int   = 50              # How many time steps to update policy
max_iter: int     = 100             # Maximum number of iterations during episode
early_stop: float = -50.0           # Early stop criterion
batch_size: int   = 10              # size of the batches during epoch
act_std: float    = 0.25            # standard deviation action distribution
rng_seed: int     = 666             # Random seed for reproducability
algorithm: str    = "ppo"           # Name of used algorithm ∈  ./algorithm
ace_id: str       = "op2"           # ACE Identifier of the Environment
ace_backend: str  = "xh035"         # PDK/Technology backend of the ACE Environment
ace_variant: int  = 0               # ACE Environment variant
ε: float          = 0.2             # Factor for clipping
η: float          = 0.001           # Factor in loss function
γ: float          = 0.99            # Discount Factor
τ: float          = 0.95            # Avantage Factor
α: float          = 1e-3            # Learning Rate
βs: tuple[float]  = (β1, β2) where: # Weight Decay
    β1: float     = 0.9
    β2: float     = 0.999

## Setup
env_id: str     = f"gace:{ace_id}-{ace_backend}-v{ace_variant}"
time_stamp: str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
model_dir: str  = f"./models/{time_stamp}-{env_id}-{algorithm}"
model_path: str = f"{model_dir}/checkpoint.pt"
log_dir: str    = f"./runs/{time_stamp}-{env_id}-{algorithm}/"
os.makedirs(model_dir, exist_ok = True)

## Setup Globals
device = pt.device("cuda:1") if pt.cuda.is_available() else pt.device("cpu")
writer = SummaryWriter(log_dir = log_dir, flush_secs = 30)
_      = rng_seed |> pt.manual_seed

## Utility
def write_performance(env: gym.Env, iteration: int) = performance where:
    target      = env.target
    performance = env |> .ace |> ac.current_performance
    for k in target.keys():
        writer.add_scalars(k , { f"Performance": performance[k]
                               , f"Target": target[k] }
                          , iteration)

## Memory
data Memory( states: pt.Tensor, actions: pt.Tensor, logprobs: pt.Tensor
           , rewards: pt.Tensor, values: pt.Tensor, masks: pt.Tensor ):
    def __add__(self, other) = (self, other) |*> zip |> fmap$(pt.cat) |*> Memory

def data_loader(memory: Memory, batch_size: int = batch_size) = loader where:
    states,actions,logprobs,rewards,values,masks \
               = memory |> fmap$(.[:-1]) |*> Memory
    values_    = memory.values[1:]
    returns    = gae(rewards, values, masks, values_)
    advantages = returns - values
    loader     = (states, actions, logprobs, returns, advantages) \
               |*> TensorDataset \
               |> DataLoader$(?, batch_size = batch_size, shuffle = True)

## Neural Networks for actor and critic
def act_net(obs_dim: int, act_dim: int) = net where:
    net = pt.nn.Sequential( pt.nn.Linear(obs_dim, 512)    , pt.nn.ReLU()
                          , pt.nn.Linear(512    , 256)    , pt.nn.ReLU()
                          , pt.nn.Linear(256    , 128)    , pt.nn.ReLU()
                          , pt.nn.Linear(128    , 64)     , pt.nn.ReLU()
                          , pt.nn.Linear(64     , act_dim), pt.nn.Tanh() )

def crt_net(obs_dim: int) = net where:
    net = pt.nn.Sequential( pt.nn.Linear(obs_dim, 512), pt.nn.ReLU()
                          , pt.nn.Linear(512    , 256), pt.nn.ReLU()
                          , pt.nn.Linear(256    , 128), pt.nn.ReLU()
                          , pt.nn.Linear(128    , 64) , pt.nn.ReLU()
                          , pt.nn.Linear(64     , 1)  )

data Model( actor: pt.nn.Module, critic: pt.nn.Module
          , optim: pt.optim.Optimizer, std: float ):
    def eval_mode(self) = self where:
        [self.actor, self.critic] |> fmap$(.to(device) .. .eval())
    def train_mode(self) = self where:
        [self.actor, self.critic] |> fmap$(.to(device) .. .train())
    def act_cont( self, states: pt.Tensor) = (π, v) where:
        μ = states |> self.actor
        σ = μ |> .shape |> .[1] |> pt.eye |> (*)$(?,self.std) |> .to(device)
        π = pt.distributions.MultivariateNormal(μ, σ)
        v = states |> self.critic |> .squeeze()
    def act_disc(self, states: pt.Tensor) = (π, v) where:
        μ = states |> self.actor
        π = μ |> pt.distributions.Categorical
        v = states |> self.critic |> .squeeze()

def make_model(act_dim: int, obs_dim: int, std: float) = model where:
    actor: pt.nn.Module       = act_net(obs_dim, act_dim) |> .to(device)
    critic: pt.nn.Module      = crt_net(obs_dim)          |> .to(device)
    policy: pt.nn.ModuleList  = pt.nn.ModuleList([actor, critic])
    optim: pt.optim.Optimizer = pt.optim.Adam(policy.parameters(), lr = α, betas = βs)
    model: Model              = Model(actor, critic, optim, std)

def update_policy( model: Model, states: pt.Tensor, actions: pt.Tensor
                 , logprobs: pt.Tensor, returns:pt.Tensor, advantages: pt.Tensor 
                 ) = losses where:
    dist,values = states |> model.act_cont
    entropy     = dist |> .entropy() |> .mean()
    logprobs_   = actions |> dist.log_prob
    ratios      = (logprobs_ - logprobs) |> pt.exp
    surr_1      = ratios * advantages
    surr_2      = pt.clamp(ratios, 1.0 - ε, 1.0 + ε) * advantages
    loss_act    = (surr_1, surr_2) |*> pt.min |> (-)
    loss_crt    = (returns - values) |> pt.pow$(?,2) |> pt.mean
    losses      = 0.5 * loss_crt + loss_act - η * entropy
    _           = model.optim       |> .zero_grad()
    _           = losses |> pt.mean |> .backward()
    _           = model.optim       |> .step()

def postprocess(observations, keys) = states where:
    idx = [ key |> filter$((k) -> ":" not in k, ?) |> map$((p) -> p |> key.index) |> list
            for key in keys ] 
    states = [ o[i] for o,i in zip(observations, idx)
             ] |> fmap$(pt.from_numpy) |> pt.vstack |> .to(device)

def trajectories( model: Model, envs: gace.envs.vec.VecACE
                , states: pt.Tensor ) = (memories, next_states) where:
    dist,values = states |> model.act_cont
    actions     = dist |> .sample() |> .detach()
    logprobs    = actions |> dist.log_prob
    observations,rewards_,dones,infos \
                = actions |> pt.split$(?,1) \
                          |> fmap$(.numpy() .. .cpu() .. .squeeze()) \
                          |> list |> envs.step
    next_states = postprocess(observations, [i["output-parameters"] for i in infos])
    masks       = dones |> pt.Tensor |> .to(device) |> (-)$(1,?)
    rewards     = rewards_ |> pt.Tensor |> .to(device)
    memories    = Memory(states,actions,logprobs,rewards,values,masks)

def gae( r: pt.Tensor, v: pt.Tensor, m: pt.Tensor, v_: pt.Tensor
       , γ: float = 0.99, τ: float = 0.95 ) = a where:
    δ = r + γ * v_ * m - v
    l = δ |> .shape |> .[0] |> range |> reversed
    i = pt.Tensor([0]).to(device)
    g = reduce( (g_,i_) -> δ[i_] + γ * τ * m[i_] * g_[0] |> (,)$(?,g_) |> pt.hstack
              , l, i ) |> .[:-1]
    a = v + g

def save_checkpoint( model: Model, checkpoint_file: str) = res where:
    state_dicts = [model.actor, model.critic, model.optim] |> fmap$(.state_dict())
    state_keys  = ["actor", "critic", "optim"]
    save_dict   = (state_keys, state_dicts) |*> zip |> dict 
    res         = pt.save(save_dict, checkpoint_file)

def traject(model, envs, (memories,states), step) = (memories_, states_) where:
    t0             = time.time()
    memory,states_ = trajectories(model, envs, states)
    memories_      = memory + memories
    t1             = time.time()
    if verbose:
        r = memory.rewards.mean()
        print(f"Step {step:03} took {(t1 - t0):.3f}s | Average Reward: {r:.3f}")
    writer.add_scalar("_Reward_Mean", memory.rewards.mean(), step)

def train(model, epoch, loader) = loss |> pt.mean where:
    t0   = time.time()
    loss = starmap(update_policy$(model), loader) |> list |> pt.cat
    t1   = time.time()
    writer.add_scalar("_Loss_Mean", loss.mean().item(), epoch)

def run_episode(_, _, _, _, _, True, loss, reward) = model where:
    total   = reward |> .sum() |> .item()
    _       = writer.add_scalar(f"_Reward_Total", total, episode)
    _       = writer.add_scalar("_Loss_Sum", loss.sum().item(), epoch)
    if verbose:
        f"Episode {episode:03} Finished | Total Reward: {total}" |> print
addpattern def run_episode(envs, model, episode, iteration, states, _, loss, reward
                          ) = run_episode( envs, model, episode, iteration_
                                         , next_states, done, loss_, reward_ 
                                         ) where:
    _          = model.eval_mode()
    with pt.no_grad(): 
        memories_,next_states \
               = reduce( traject$(model, envs), range(num_steps)
                       , ( pt.empty(0) |> .to(device) |> repeat |> .$[:6] |*> Memory
                         , states ))
    _          = envs |> .gace_envs |> .[0] |> write_performance$(?, iteration )
    _          = model.train_mode()
    loader     = data_loader(memories_)
    losses     = [ train(model, epoch, loader) for epoch in num_epochs |> range 
                 ] |> pt.stack  |> .detach()
    done_      = .item() .. pt.all .. .bool() <| (1 - memories_.masks)
    stop_      = (memories_.rewards |> pt.mean |> .item()) < early_stop
    done       = done_ or stop_ or (iteration >= max_iter)
    loss_      = pt.cat((loss, losses))
    reward_    = pt.cat((reward, memories_.rewards))
    iteration_ = iteration + 1
    if verbose:
        r = memories_.rewards.mean().item()
        print(f"Epsiode {episode:03}, Iteration {iteration:03} | Average Reward {r:.3f}")
    _          = save_checkpoint(model, model_path)

def run_episodes( model: Model, envs: gace.envs.vec.VecACE, episode: int
                ) = model where:
    obs     = envs.reset()
    states  = postprocess(obs, envs.observation_keys)
    l = r = pt.empty(0) |> .to(device = device)
    losses,rewards = run_episode(envs, model, episode, 0, states, False, l, r)
