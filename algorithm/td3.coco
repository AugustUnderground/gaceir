import coconut.convenience

import os, time, datetime
from collections import namedtuple
from itertools import repeat
import torch as pt
from torch.utils.data import TensorDataset, DataLoader
import torch_optimizer as optim
from torch.utils.tensorboard import SummaryWriter
from fastprogress.fastprogress import master_bar, progress_bar
import gym, gace
import hace as ac

## Defaults (as args later)
verbose: bool        = True            # Print verbose debug output
#num_envs: int        = os.sched_getaffinity(0) |> len |> (//)$(?,2)
num_envs: int        = 42              # Number of parallel env pool
num_episodes: int    = 42              # Number of episodes to play
num_steps: int       = 25              # How many steps to take -> num_steps × num_envs = n_points ∈  data_set
max_steps: int       = 100             # How many steps to take -> num_steps × num_envs = n_points ∈  data_set
num_epochs: int      = 100             # How many time steps to update policy
early_stop: float    = -50.0           # Early stop criterion
batch_size: int      = 256             # size of the batches during epoch
act_std: float       = 0.25            # standard deviation action distribution
rng_seed: int        = 666             # Random seed for reproducability
algorithm: str       = "td3"           # Name of used algorithm ∈  ./algorithm
ace_id: str          = "op2"           # ACE Identifier of the Environment
ace_backend: str     = "xh035"         # PDK/Technology backend of the ACE Environment
ace_variant: int     = 0               # ACE Environment variant
γ: float             = 0.99            # Discount Factor
τ_soft: float        = 1e-2            # Avantage Factor
α: float             = 1e-4            # Learning Rate
βs: tuple[float]     = (β1, β2) where: # Weight Decay
    β1: float        = 0.9
    β2: float        = 0.999
exp_noise: float     = 0.15
pol_noise: float     = 0.2
clp_noise: float     = 0.5
update_interval: int = 2
buffer_size: int     = 1e7 |> int
warmup_periode: int  = 100

## Setup
env_id: str     = f"gace:{ace_id}-{ace_backend}-v{ace_variant}"
time_stamp: str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
model_dir: str  = f"./models/{time_stamp}-{env_id}-{algorithm}"
model_path: str = f"{model_dir}/checkpoint.pt"
log_dir: str    = f"./runs/{time_stamp}-{env_id}-{algorithm}/"
os.makedirs(model_dir, exist_ok = True)

## Setup Globals
device = pt.device("cuda:1") if pt.cuda.is_available() else pt.device("cpu")
writer = SummaryWriter(log_dir = log_dir, flush_secs = 30)
_      = rng_seed |> pt.manual_seed

## Environment setup
envs: gace.envs.vec.VecACE = gace.vector_make_same(env_id, num_envs)
#obs_dim: int = envs.observation_space[0].shape[0]
obs_dim: int = envs$[0].target |> len |> (*)$(3)
#obs_dim: int = envs$[0].target |> len |> (*)$(2)
act_dim: int = envs.action_space[0].shape[0]

q_loss = pt.nn.MSELoss()

## Utility
def write_performance(env: gym.Env, step: int) = performance where:
    target      = env.target
    performance = env |> .ace |> ac.current_performance
    for k in target.keys():
        writer.add_scalars( k, { "performance": performance[k]
                               , "target": target[k] }
                          , step )

## Replay Buffer
ReplayBuffer         = namedtuple("ReplayBuffer", "state action reward next_state done")

def push(b: ReplayBuffer, t: tuple) = b_ where:
    b_ = (if len(b.state) <= 0 
             then t |*> ReplayBuffer |> fmap$(.[-buffer_size:])
             else zip(b,t) |> fmap$(.[-buffer_size:] .. pt.vstack) |*> ReplayBuffer)

def sample(b: ReplayBuffer, bs: int) = s where:
    i = b.state |> .shape |> .[0] |> pt.randperm |> .[:bs] |> .tolist()
    s = (e[i,:] for e in b) |*> ReplayBuffer

## Neural Networks
data HStack() from pt.nn.Module:
    def forward(self, X: tuple[pt.Tensor, pt.Tensor]) = X |> pt.hstack

data VStack() from pt.nn.Module:
    def forward(self, X: tuple[pt.Tensor, pt.Tensor]) = X |> pt.vstack

## Critic
def critic_net(obs_dim: int, act_dim: int) = crt_net where:
    dim = obs_dim + act_dim
    crt_net = pt.nn.Sequential( HStack()
                              , pt.nn.Linear(dim  , 256), pt.nn.ReLU()
                              , pt.nn.Linear(256, 128), pt.nn.ReLU()
                              , pt.nn.Linear(128, 64) , pt.nn.ReLU()
                              , pt.nn.Linear(64 , 1)  )

## Actor
def actor_net(obs_dim: int, act_dim: int) = act_net where:
    act_net = pt.nn.Sequential( pt.nn.Linear(obs_dim, 128)    , pt.nn.ReLU()
                              , pt.nn.Linear(128    , 256)    , pt.nn.ReLU()
                              , pt.nn.Linear(256    , 128)    , pt.nn.ReLU()
                              , pt.nn.Linear(128    , 64)     , pt.nn.ReLU()
                              , pt.nn.Linear(64     , act_dim), pt.nn.Tanh())

def soft_update(source, target, τ = 1e-2) = target where:
    for param, target_param in zip(source.parameters(), target.parameters()):
        target_param.data.copy_(τ * param.data + (1 - τ) * target_param.data)

## Current Networks
Q1 = critic_net(obs_dim, act_dim) |> .to(device)
Q2 = critic_net(obs_dim, act_dim) |> .to(device)
π  = actor_net(obs_dim, act_dim)  |> .to(device)

## Target Networks
θ1 = critic_net(obs_dim, act_dim) |> .to(device)
θ2 = critic_net(obs_dim, act_dim) |> .to(device)
φ  = actor_net(obs_dim, act_dim)  |> .to(device)

## Initial Syncronization
_ = soft_update(Q1, θ1, τ = 1.0)
_ = soft_update(Q2, θ2, τ = 1.0)
_ = soft_update(π, φ, τ = 1.0)

## Optimizers
q1_optim = pt.optim.Adam(Q1.parameters(), lr = α, betas = βs)
q2_optim = pt.optim.Adam(Q2.parameters(), lr = α, betas = βs)
π_optim  = pt.optim.Adam(π.parameters(), lr = α, betas = βs)

## Exploitation
def add_noise( a, t = 0, lo = -1.0, hi = 1.0, σ_max = 1.0, σ_min = 1.0, d = 1000000
             ) = a_ where:
    σ  = σ_max - (σ_max - σ_min) * min(1.0, t / d)
    a_ = (a + pt.randn_like(a) * σ) |> pt.clip$(?, min = lo, max = hi)

def random_action(envs) = action where:
    action = envs.action_space |> fmap$(pt.from_numpy .. .sample()) \
                               |> pt.vstack |> .to(device)

def select_action(net, state, step) = action where:
    with pt.no_grad():
        action = state |> pt.Tensor |> .unsqueeze(0) |> .to(device) |> net \
                       |> .detach() |> add_noise$(?,t = step) |> .cpu() |> .numpy() |> .[0]

def policy_update(epoch, states) = π_loss where:
    π_loss = states |> π |> (,)$(states,?) |> Q1 |> .mean() |> (-)
    _      = π_optim.zero_grad()
    _      = π_loss.backward()
    _      = π_optim.step()
    _      = soft_update(Q1, θ1, τ = τ_soft)
    _      = soft_update(Q2, θ2, τ = τ_soft)
    _      = soft_update(π , φ , τ = τ_soft)
    _      = writer.add_scalar("π_loss" , π_loss , epoch)
    if verbose:
        print(f"Actor Update {epoch:03}/{num_epochs}  | π Loss: {π_loss.item():.3f}")

def update(epoch, buffer) = (q1_loss, q2_loss, π_loss) where:
    states, actions, rewards, next_states, dones \
                 = sample(buffer, batch_size) |> fmap$(.to(device))
    noise        = actions.shape |> pt.zeros |> pt.normal$(?,pol_noise) \
                                 |> pt.clamp$(?,-clp_noise, clp_noise)  \
                                 |> .to(device)
    next_actions = states |> φ |> (+)$(?,noise) |> pt.clamp$(?, -1.0, 1.0)
    q1_target    = (next_states, next_actions) |> θ1
    q2_target    = (next_states, next_actions) |> θ2
    q_target     = pt.min(q1_target, q2_target)
    q_expected   = (rewards + (1.0 - dones) * γ * q_target) |> .detach()
    q1           = (states, actions) |> Q1
    q2           = (states, actions) |> Q2
    q1_loss      = q_loss(q1, q_expected)
    q2_loss      = q_loss(q2, q_expected)
    _            = q1_optim.zero_grad()
    _            = q1_loss.backward()
    _            = q1_optim.step()
    _            = q2_optim.zero_grad()
    _            = q2_loss.backward()
    _            = q2_optim.step()
    π_loss       = (if epoch in count(0, update_interval) 
                       then policy_update(epoch, states)
                       else pt.nan |> pt.tensor |> .to(device))
    _            = writer.add_scalar("q1_loss", q1_loss, epoch)
    _            = writer.add_scalar("q2_loss", q2_loss, epoch)
    if verbose:
        print(f"Critic Update {epoch:03}/{num_epochs} | Q1 Loss: {q1_loss.item():.3f}")
        print(f"                      | Q2 Loss: {q2_loss.item():.3f}")

## Exploration
def postprocess(observations, infos) = states where:
    #performances = subset(observations, infos, "performance") \
    #             / subset(observations, infos, "target")
    #distances    = subset(observations, infos, "distance")
    #states       = (performances, distances) |> pt.hstack
    states = [ [ i["output-parameters"].index(p) for p in i["output-parameters"] 
                 if ["performance", "target", "distance"] |> fmap$(p.startswith, ?) |> any 
               ] for i in infos 
             ] |> zip$(?,observations) |> starmap$(pt.from_numpy .. ((i,o) -> o[i])) \
               |> list |> pt.vstack |> .to(device) |> .detach()
    #states = observations |> fmap$pt.from_numpy |> pt.vstack |> .to(device) |> .detach() 

def explore( envs, step, states, buffer
           ) = (next_states, new_buffer) where:
    actions     = (if (step * num_envs) < warmup_periode
                      then random_action(envs)
                      else select_action(π, states, step))
    t0          = time.time()
    observations,rewards_,dones_,infos \
                = actions |> pt.split$(?,1) \
                          |> fmap$(.numpy() .. .cpu() .. .squeeze()) \
                          |> list |> envs.step
    next_states = postprocess(observations, infos)
    rewards     = rewards_ |> pt.Tensor |> .to(device) |> .reshape(-1,1)
    dones       = dones_ |> pt.Tensor |> .to(device) |> .reshape(-1,1)
    new_buffer  = push(buffer, (states, actions, rewards, next_states, dones))
    t1          = time.time()
    _           = writer.add_scalar("Total_Reward", rewards.sum().item(), step)
    _           = envs$[0] |> write_performance$(?, step)
    if verbose:
        print(f"Step {step:03}/{num_steps} took {(t1 - t0):.3f}s | Average Reward: {rewards.mean():.3f}")

## Run Episode until done
def run_episode(_, _, buffer, True) = buffer
addpattern def run_episode( episode, states, buffer, done
                          ) = run_episode( episode, next_states, new_buffer
                                         , done_ or stop_ ) where:
    _      = [π, Q1, Q2, φ, θ1, θ2] |> fmap$(.eval)
    expl   = (sb, e) -> sb |*> explore$(envs, e, ?)
    next_states, new_buffer \
           = reduce(expl, num_steps |> range, (states, buffer))
    done_  = .item() .. pt.all .. .bool() .. .squeeze() .. .done <| new_buffer
    stop_  = False #new_buffer.rewards[-num_envs:].mean().item() < early_stop
    _      = [π, Q1, Q2, φ, θ1, θ2] |> fmap$(.train)
    q1_loss, q2_loss, π_loss_ \
           = [ update(epoch, new_buffer) for epoch in num_epochs |> range 
             ]  |*> zip |> fmap$pt.hstack |> tuple
    π_loss = π_loss_[~π_loss_.isnan()]

def run_episodes(episode: int) = memory where:
    states = envs.reset() |> fmap$(.[:obs_dim] .. pt.from_numpy) |> pt.vstack |> .to(device)
    buffer = pt.empty(0) |> repeat |> .$[:5] |> list |*> ReplayBuffer
    memory = run_episode(episode, states, buffer, False)

